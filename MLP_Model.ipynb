{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (0.8.7)\n",
      "Requirement already satisfied: lxml>=2.3.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from python-docx) (4.2.1)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (3.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: keras in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.14.5)\n",
      "Requirement already satisfied: keras-preprocessing==1.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.0.2)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: keras-applications==1.0.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.0.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: gensim in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (3.6.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from gensim) (1.14.5)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (1.7.82)\n",
      "Requirement already satisfied: bz2file in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
      "Requirement already satisfied: boto>=2.32 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (2.48.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
      "Requirement already satisfied: botocore<1.11.0,>=1.10.82 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim) (1.10.84)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (2018.8.24)\n",
      "Requirement already satisfied: docutils>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.82->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.82->boto3->smart-open>=1.2.1->gensim) (2.7.3)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorflow in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (1.10.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied: setuptools<=39.1.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (39.1.0)\n",
      "Requirement already satisfied: numpy<=1.14.5,>=1.13.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (1.14.5)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (1.10.1)\n",
      "Requirement already satisfied: tensorboard<1.11.0,>=1.10.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (1.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (0.5.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow) (3.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: sklearn in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sklearn) (0.19.1)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx\n",
    "!pip install nltk\n",
    "!pip install keras\n",
    "!pip install gensim\n",
    "!pip install tensorflow\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Data From S3 Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "bucket = \"projectworks\"\n",
    "file_name = \"Cancer.csv\"\n",
    "\n",
    "s3 = boto3.client('s3') \n",
    "\n",
    "obj = s3.get_object(Bucket= bucket, Key= file_name) \n",
    "\n",
    "\n",
    "initial_df = pd.read_csv(obj['Body'])\n",
    "\n",
    "data=initial_df[\"Description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca=[]\n",
    "text=''\n",
    "temp_text=''\n",
    "ct=0\n",
    "l = list(data)\n",
    "for i in l:\n",
    "    text = text+\"\"+i\n",
    "    voca.append(i)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_y(voca):\n",
    "    c=len(voca)\n",
    "    y=[1 for i in range(0,c)]\n",
    "    for i in range(0,c) :\n",
    "        Word=nltk.word_tokenize(voca[i])\n",
    "        if 'II' in Word:\n",
    "            y[i]=2\n",
    "        if 'III' in Word:\n",
    "            y[i]=3\n",
    "        if 'IV' in Word:\n",
    "            y[i]=4\n",
    "        if 'V' in Word:\n",
    "            y[i]=5\n",
    "        if 'VI' in Word:\n",
    "            y[i]=6    \n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(doc_words,doc_temp,vocabulary):    # boolean representation \n",
    "    bool_vec = []\n",
    "    temp_vec=[]\n",
    "      \n",
    "    for i in range(len(doc_words)):\n",
    "        for j in range(len(vocabulary)):\n",
    "            if vocabulary[j] in doc_words[i]:\n",
    "               temp_vec.append(1)\n",
    "            else:\n",
    "                temp_vec.append(0)      \n",
    "        \n",
    "        bool_vec.append(temp_vec)       \n",
    "        temp_vec=[]  \n",
    "        \n",
    "    return bool_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_text(text):\n",
    "    w_text = nltk.word_tokenize(text)\n",
    "    \n",
    "    punc_text=[]\n",
    "    for i in w_text:\n",
    "        if i not in string.punctuation:\n",
    "            punc_text.append(i) \n",
    "\n",
    "    for i in punc_text:\n",
    "        i=i.lower()\n",
    "    \n",
    "    stop_words=set(stopwords.words('english'))\n",
    "\n",
    "    non_stop_text=[]        \n",
    "    for i in punc_text:\n",
    "        if i not in stop_words:\n",
    "            non_stop_text.append(i)\n",
    "    \n",
    "    unique_text = list(set(non_stop_text))\n",
    "\n",
    "    return unique_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_words_cleaning(voca):       # cleans and preprocesses each document \n",
    "    doc_words = []    \n",
    "    for i in range(len(voca)):\n",
    "        doc_words.append(clean_text(voca[i]))\n",
    "    return doc_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(doc_words,doc_temp,vocabulary):    # boolean representation \n",
    "    bool_vec = []\n",
    "    temp_vec=[]\n",
    "      \n",
    "    for i in range(len(doc_words)):\n",
    "        for j in range(len(vocabulary)):\n",
    "            if vocabulary[j] in doc_words[i]:\n",
    "               temp_vec.append(1)\n",
    "            else:\n",
    "                temp_vec.append(0)      \n",
    "        \n",
    "        bool_vec.append(temp_vec)       \n",
    "        temp_vec=[]  \n",
    "        \n",
    "    return bool_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_term_freq(boo):\n",
    "    doc_freq=[]\n",
    "    for i in range(len(boo)):\n",
    "        summ=0\n",
    "        for j in range(len(boo[0])):\n",
    "            if boo[i][j] == 1:\n",
    "                summ=summ+1\n",
    "        doc_freq.append(summ)\n",
    "    return doc_freq\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of no of vocabulary words in each document\n",
    "\n",
    "def count_voca_words(bove):\n",
    "    a=[]\n",
    "    for i in range(len(bove)):\n",
    "        a.append(bove[i].count(1))    \n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term frequency [NAIVE] :-  \n",
    "\n",
    "def term_frequency_naive(doc_words,vocabulary):   \n",
    "    tf=[]\n",
    "    temp=[]\n",
    "    for i in range(len(doc_words)):\n",
    "        for j in vocabulary:\n",
    "            temp.append(doc_words[i].count(j))\n",
    "        tf.append(list(zip(temp,vocabulary)))\n",
    "        temp=[]\n",
    "    \n",
    "    return tf            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf - idf term frequency \n",
    "\n",
    "def tf(doc_words,vocabulary):\n",
    "    tf=[]\n",
    "    temp=[]\n",
    "    for i in range(len(doc_words)):\n",
    "        for j in vocabulary:\n",
    "            temp.append((doc_words[i].count(j))/len(doc_words[i]))\n",
    "        tf.append(temp)\n",
    "        temp=[]\n",
    "    return tf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(doc_words,vocabulary):\n",
    "    import math\n",
    "    doc_count = 0\n",
    "    idf=[]\n",
    "    for i in vocabulary:\n",
    "        for j in range(len(doc_words)):\n",
    "            if i in doc_words[j]:\n",
    "                doc_count = doc_count + 1\n",
    "        idf.append(math.log(len(doc_words)/(doc_count)))\n",
    "        doc_count = 0 \n",
    "    \n",
    "    return idf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_score_calc(tf,idf):\n",
    "    tf_idf = []\n",
    "    temp=[]\n",
    "    for i in range(len(tf)):\n",
    "        for j in range(len(idf)):\n",
    "            if tf[i][j] !=0 and idf[j] !=0:\n",
    "                temp.append(tf[i][j]*idf[j])\n",
    "            else:\n",
    "                temp.append(0)\n",
    "        tf_idf.append(temp)\n",
    "        temp=[]\n",
    "    \n",
    "    return tf_idf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = clean_text(text)\n",
    "\n",
    "doc_words = doc_words_cleaning(voca)\n",
    "\n",
    "bow = bag_of_words(doc_words,voca,vocabulary)\n",
    "\n",
    "y=find_y(voca)\n",
    "\n",
    "\n",
    "doc_freq = count_voca_words(bow)   \n",
    "\n",
    "term_freq_naive = term_frequency_naive(doc_words,vocabulary)\n",
    "\n",
    "term_frequency = tf(doc_words,vocabulary)\n",
    "\n",
    "inverse_doc_freq = idf(doc_words,vocabulary)\n",
    "\n",
    "tf_idf_score = tf_idf_score_calc(term_frequency,inverse_doc_freq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "import tensorflow as tfl\n",
    "#Parameters\n",
    "L_rate=0.1\n",
    "epochs=100000000\n",
    "\n",
    "n_input=784\n",
    "n_hidden=200\n",
    "n_output=6\n",
    "\n",
    "X=tfl.placeholder(\"float\",[None,n_input])\n",
    "Y=tfl.placeholder(\"float\",[None,n_output])\n",
    "\n",
    "w1=tfl.Variable(tfl.random_normal([n_input,n_hidden]))\n",
    "w2=tfl.Variable(tfl.random_normal([n_hidden,n_output]))\n",
    "\n",
    "bias1=tfl.Variable(tfl.random_normal([n_hidden]))\n",
    "bias2=tfl.Variable(tfl.random_normal([n_output]))\n",
    "\n",
    "def model(X):\n",
    "  layer1=tfl.add(tfl.matmul(X,w1),bias1)\n",
    "  layer1=tfl.nn.relu(layer1)\n",
    "  output_layer=tfl.matmul(layer1,w2)+bias2\n",
    "  return output_layer\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bilding The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow, dummy_y)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(term_frequency, dummy_y)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(tf_idf_score, dummy_y)\n",
    "\n",
    "pred=model(X)\n",
    "cost=tfl.reduce_mean(tfl.nn.softmax_cross_entropy_with_logits(logits=pred,labels=Y))\n",
    "optimizer =tfl.train.AdamOptimizer(L_rate).minimize(cost)\n",
    "init=tfl.global_variables_initializer()\n",
    "\n",
    "with tfl.Session() as sess:\n",
    "  sess.run(init)\n",
    "  for epochs in range(epochs):\n",
    "    _.c=sess.run([optimizer,cost],feed_dict={X:X_train,Y:y_train})\n",
    "    \n",
    "  test_result = sess.run(pred,feed_dict= {X:X_train})\n",
    "  y_pred=tfl.equal(tfl.argmax(test_result,1),tfl.argmax(y_train,1))\n",
    "  accuracy=tfl.reduce_mean(tfl.cast(y_pred,\"float\"))\n",
    "  \n",
    "  print(\"Accuracy:\",accuracy.eval({X:X_test,Y:y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.13\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(bow, dummy_y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(term_frequency, dummy_y)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(tf_idf_score, dummy_y)\n",
    "\n",
    "pred=model(X)\n",
    "cost=tfl.reduce_mean(tfl.nn.softmax_cross_entropy_with_logits(logits=pred,labels=Y))\n",
    "optimizer =tfl.train.AdamOptimizer(L_rate).minimize(cost)\n",
    "init=tfl.global_variables_initializer()\n",
    "\n",
    "with tfl.Session() as sess:\n",
    "  sess.run(init)\n",
    "  for epochs in range(epochs):\n",
    "    _.c=sess.run([optimizer,cost],feed_dict={X:X_train,Y:y_train})\n",
    "    \n",
    "  test_result = sess.run(pred,feed_dict= {X:X_train})\n",
    "  y_pred=tfl.equal(tfl.argmax(test_result,1),tfl.argmax(y_train,1))\n",
    "  accuracy=tfl.reduce_mean(tfl.cast(y_pred,\"float\"))\n",
    "  \n",
    "  print(\"Accuracy:\",accuracy.eval({X:X_test,Y:y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.28\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(bow, dummy_y)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(term_frequency, dummy_y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_score, dummy_y)\n",
    "\n",
    "pred=model(X)\n",
    "cost=tfl.reduce_mean(tfl.nn.softmax_cross_entropy_with_logits(logits=pred,labels=Y))\n",
    "optimizer =tfl.train.AdamOptimizer(L_rate).minimize(cost)\n",
    "init=tfl.global_variables_initializer()\n",
    "\n",
    "with tfl.Session() as sess:\n",
    "  sess.run(init)\n",
    "  for epochs in range(epochs):\n",
    "    _.c=sess.run([optimizer,cost],feed_dict={X:X_train,Y:y_train})\n",
    "    \n",
    "  test_result = sess.run(pred,feed_dict= {X:X_train})\n",
    "  y_pred=tfl.equal(tfl.argmax(test_result,1),tfl.argmax(y_train,1))\n",
    "  accuracy=tfl.reduce_mean(tfl.cast(y_pred,\"float\"))\n",
    "  \n",
    "  print(\"Accuracy:\",accuracy.eval({X:X_test,Y:y_test}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
